"
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Markdown to HTML</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/5.1.0/github-markdown.min.css">
    <style>
        .markdown-body {
            box-sizing: border-box;
            min-width: 200px;
            max-width: 980px;
            margin: 0 auto;
            padding: 45px;
        }
    </style>
</head>
<body>
    <div class="markdown-body">
        <h3 id="processing-stream-data-on-the-consumer-side">Processing Stream Data on the Consumer Side</h3>
<p>On the consumer side in streaming systems (e.g., Kafka, Kinesis), processing involves reading events from a stream, applying transformations (like filtering, joining, or aggregating), and producing outputs (e.g., to another stream, database, or dashboard). This is often done with stream processing libraries that handle state management, fault tolerance, and exactly-once semantics. The consumer pulls data via offsets, processes in micro-batches or continuously, and commits progress.</p>
<p>Key benefits: Real-time insights (e.g., live dashboards), scalability (parallel consumers), and durability (replay from offsets). Libraries like <strong>Kafka Streams</strong>, <strong>Apache Flink</strong>, or <strong>Spark Structured Streaming</strong> abstract the boilerplate, treating streams as unbounded tables for SQL-like ops.</p>
<h4 id="high-level-steps-for-consumer-side-processing">High-Level Steps for Consumer-Side Processing</h4>
<ol>
<li><strong>Setup Consumer</strong>: Connect to the stream (e.g., specify topic, group ID for load balancing).</li>
<li><strong>Read Events</strong>: Poll for records; deserialize (e.g., JSON to objects).</li>
<li><strong>Transform/Aggregate</strong>: Apply logic—e.g., filter events, join with another stream, or window for aggregations.</li>
<li><strong>State Management</strong>: Use in-memory stores (e.g., RocksDB in Kafka Streams) for aggregations; checkpoint for recovery.</li>
<li><strong>Output</strong>: Sink results (e.g., to Elasticsearch); commit offsets.</li>
<li><strong>Handle Faults</strong>: Use retries, dead-letter topics, and watermarks for late events.</li>
</ol>
<h4 id="example-time-windows-and-aggregations">Example: Time Windows and Aggregations</h4>
<p>Time windows group events into time-based buckets for computations like sums or averages—essential for metrics like &quot;hourly sales total.&quot; Windows can be:</p>
<ul>
<li><strong>Tumbling</strong>: Non-overlapping (e.g., fixed 5-min slots).</li>
<li><strong>Sliding</strong>: Overlapping (e.g., 5-min window every 1 min).</li>
<li><strong>Session</strong>: Dynamic, based on inactivity gaps.</li>
</ul>
<p><strong>Scenario</strong>: Process a stream of e-commerce transactions (events: <code>{user_id, timestamp, amount}</code>) to compute rolling 10-min average order value.</p>
<ul>
<li><p><strong>Using Kafka Streams (Java-like pseudocode)</strong>:</p>
<pre><code class="language-java:disable-run">StreamsBuilder builder = new StreamsBuilder();
KStream&lt;String, Transaction&gt; stream = builder.stream(&quot;transactions-topic&quot;)
    .mapValues(value -&gt; new Transaction(value.getUserId(), value.getTimestamp(), value.getAmount()));

// Time window aggregation: Sliding 10-min window, every 1 min
KTable&lt;Windowed&lt;String&gt;, Double&gt; avgOrders = stream
    .filter((key, value) -&gt; value.amount &gt; 0)  // Filter valid txns
    .groupByKey()  // Group by user_id (key)
    .windowedBy(TimeWindows.of(Duration.ofMinutes(10)).advanceBy(Duration.ofMinutes(1)))  // Sliding window
    .aggregate(
        () -&gt; 0.0,  // Initial value
        (key, value, aggregate) -&gt; aggregate + value.amount,  // Add to sum
        Materialized.with(Serdes.String(), Serdes.Double()))  // State store
    .toStream()  // Convert back to stream
    .mapValues(avg -&gt; avg / 10);  // Simple avg (in real: track count too)

avgOrders.toStream().to(&quot;avg-orders-output-topic&quot;);  // Sink to new topic
KafkaStreams streams = new KafkaStreams(builder.build(), props);
streams.start();
</code></pre>
<ul>
<li><strong>What Happens</strong>: Events are keyed by <code>user_id</code>, windowed by event time (or processing time), aggregated in state stores, and output every advance interval. For late events, use allowed lateness to include or drop them.</li>
</ul>
</li>
<li><p><strong>Using Apache Flink (Scala-like pseudocode)</strong>:</p>
<pre><code class="language-scala">val env = StreamExecutionEnvironment.getExecutionEnvironment
val stream = env.addSource(new FlinkKafkaConsumer[String](&quot;transactions&quot;, schema, props))
    .map(_.split(&quot;,&quot;))  // Parse to (user_id, ts, amount)

val windowedStream = stream
    .keyBy(_._1)  // Key by user_id
    .window(SlidingEventTimeWindows.of(Time.minutes(10), Time.minutes(1)))
    .aggregate(new AvgAggregator())  // Custom aggregator for average

windowedStream.addSink(new FlinkKafkaProducer(&quot;avg-orders-output&quot;))
env.execute(&quot;Order Analytics&quot;)
</code></pre>
<ul>
<li><strong>Why Flink?</strong>: Excels in low-latency, stateful processing with watermarks for out-of-order events.</li>
</ul>
</li>
</ul>
<h4 id="pros-cons-and-best-practices">Pros, Cons, and Best Practices</h4>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Time Windows</strong></td>
<td>Pros: Intuitive for metrics; Cons: Sensitive to clock skew—use event time.</td>
</tr>
<tr>
<td><strong>Aggregations</strong></td>
<td>Pros: Reduces data volume; Cons: State explosion—evict old windows.</td>
</tr>
<tr>
<td><strong>Scaling</strong></td>
<td>Parallelize by keys/partitions; monitor state size.</td>
</tr>
<tr>
<td><strong>Error Handling</strong></td>
<td>Watermarks for late data; side outputs for anomalies.</td>
</tr>
</tbody>
</table>
<p><strong>Best Practices</strong>:</p>
<ul>
<li><strong>Event Time vs. Processing Time</strong>: Prefer event time (embedded timestamps) for accuracy.</li>
<li><strong>Idempotency</strong>: Use dedup keys for retries.</li>
<li><strong>Testing</strong>: Use embedded Kafka for unit tests; inject delays for window validation.</li>
<li><strong>Monitoring</strong>: Track consumer lag, window counts; tools like Prometheus.</li>
</ul>
<p>In an interview: &quot;For aggregations, I'd choose Kafka Streams for simplicity in Kafka ecosystems; Flink for complex joins. Always profile state usage to avoid OOMs.&quot; What's next—maybe joining streams or stateful processing?</p>
<pre><code></code></pre>

    </div>
</body>
</html>"