"
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Markdown to HTML</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/5.1.0/github-markdown.min.css">
    <style>
        .markdown-body {
            box-sizing: border-box;
            min-width: 200px;
            max-width: 980px;
            margin: 0 auto;
            padding: 45px;
        }
    </style>
</head>
<body>
    <div class="markdown-body">
        <h3 id="horizontal-scaling-in-kubernetes-overview">Horizontal Scaling in Kubernetes: Overview</h3>
<p>Horizontal scaling (or &quot;scaling out&quot;) in Kubernetes involves increasing the number of pod instances to distribute load across more resources, improving availability and performance without changing the app's resource requests. This is ideal for stateless .NET apps like ASP.NET Core APIs, where you can spin up replicas to handle traffic spikes. Kubernetes handles it declaratively via <strong>Deployments</strong> (or other controllers) and automates it with the <strong>Horizontal Pod Autoscaler (HPA)</strong>.</p>
<p>Vertical scaling (more CPU/memory per pod) is also possible but less flexible—horizontal is the K8s sweet spot for elasticity. Scaling is namespaced, monitored via metrics, and integrates with Services for load balancing.</p>
<h4 id="key-components-for-horizontal-scaling">Key Components for Horizontal Scaling</h4>
<ol>
<li><strong>Manual Scaling</strong>: Directly adjust replicas in a Deployment.</li>
<li><strong>Horizontal Pod Autoscaler (HPA)</strong>: Automatically scales based on metrics like CPU utilization or custom app metrics.</li>
<li><strong>Cluster Autoscaler</strong>: Scales the underlying nodes (e.g., in AKS) when pods can't schedule due to resource shortages—pairs with HPA for end-to-end elasticity.</li>
</ol>
<h4 id="step-by-step-how-to-horizontally-scale">Step-by-Step: How to Horizontally Scale</h4>
<h5 id="manual-scaling-quick-and-simple">1. <strong>Manual Scaling (Quick and Simple)</strong></h5>
<ul>
<li>Use a Deployment to define your app's desired replicas.</li>
<li>Scale via CLI or YAML updates.</li>
</ul>
<p><strong>Example YAML</strong> (Deployment for a .NET API with initial 3 replicas):</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: dotnet-api
spec:
  replicas: 3  # Initial scale
  selector:
    matchLabels:
      app: dotnet-api
  template:
    metadata:
      labels:
        app: dotnet-api
    spec:
      containers:
      - name: api
        image: mcr.microsoft.com/dotnet/aspnet:8.0
        resources:
          requests:
            cpu: &quot;100m&quot;  # 0.1 core per pod
            memory: &quot;128Mi&quot;
          limits:
            cpu: &quot;500m&quot;
            memory: &quot;256Mi&quot;
        ports:
        - containerPort: 8080
</code></pre>
<p><strong>Scale Commands</strong>:</p>
<ul>
<li>Scale up: <code>kubectl scale deployment dotnet-api --replicas=5 -n default</code></li>
<li>Scale down: <code>kubectl scale deployment dotnet-api --replicas=1 -n default</code></li>
<li>Check: <code>kubectl get deployments</code> or <code>kubectl get pods -l app=dotnet-api</code></li>
</ul>
<p><strong>When?</strong> For predictable loads, like during a known promo event for a .NET e-commerce app.</p>
<h5 id="automatic-scaling-with-hpa">2. <strong>Automatic Scaling with HPA</strong></h5>
<p>HPA watches metrics from the Metrics Server (install via <code>kubectl apply -f components.yaml</code> from K8s repo) and adjusts replicas based on targets. It supports CPU/memory (built-in) or custom metrics (via Prometheus Adapter).</p>
<p><strong>HPA Types</strong>:
| Metric Type | Description | Setup | Use Case |
|-------------|-------------|-------|----------|
| <strong>CPU/Memory</strong> | Scales on average utilization % across pods (e.g., target 50% CPU). | Metrics Server enabled. | Basic .NET API under variable HTTP load—scales on CPU spikes. |
| <strong>Custom Metrics</strong> | App-specific (e.g., requests/sec via Prometheus). | Install Prometheus + Adapter. | .NET queue processor scaling on pending jobs count. |
| <strong>External Metrics</strong> | From outside K8s (e.g., AWS SQS queue length). | Adapter for cloud providers. | .NET Worker Service pulling from Azure Service Bus. |</p>
<p><strong>Example YAML</strong> (HPA for the .NET Deployment above, scaling on CPU):</p>
<pre><code class="language-yaml">apiVersion: autoscaling/v2  # v2 for mixed metrics
kind: HorizontalPodAutoscaler
metadata:
  name: dotnet-api-hpa
spec:
  scaleTargetRef:           # References the Deployment
    apiVersion: apps/v1
    kind: Deployment
    name: dotnet-api
  minReplicas: 2            # Floor
  maxReplicas: 10           # Ceiling
  metrics:                  # Scaling criteria
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization  # Or &quot;AverageValue: 200m&quot;
        averageUtilization: 50  # Scale if avg CPU &gt;50%
</code></pre>
<p>Apply: <code>kubectl apply -f hpa.yaml</code>. Monitor: <code>kubectl get hpa</code> or <code>kubectl describe hpa dotnet-api-hpa</code> (shows current vs. target replicas).</p>
<p><strong>Behavior</strong>: If CPU averages 60% across 3 pods, HPA calculates needed replicas (e.g., to 5) and updates the Deployment. Cooldown: 3-5 mins to avoid flapping.</p>
<h5 id="node-scaling-with-cluster-autoscaler">3. <strong>Node Scaling with Cluster Autoscaler</strong></h5>
<ul>
<li>When HPA wants more pods but nodes lack resources, Cluster Autoscaler adds nodes (cloud-only).</li>
<li>Install: Provider-specific (e.g., <code>cluster-autoscaler</code> Helm chart for AKS).</li>
<li>Use Case: During Black Friday, auto-add VMs for your .NET app cluster.</li>
</ul>
<h4 id="use-cases">Use Cases</h4>
<ul>
<li><strong>Web Traffic Spikes</strong>: HPA on CPU for a .NET Blazor app—scales from 5 to 20 pods during peak hours, saving costs by downscaling at night.</li>
<li><strong>Batch Processing</strong>: Custom metric HPA on queue length for a .NET Hangfire job processor—handles bursts without over-provisioning.</li>
<li><strong>Multi-Tenant Apps</strong>: Manual scaling per namespace for isolated .NET tenants, with quotas to prevent one from starving others.</li>
<li><strong>Global Distribution</strong>: Combine with topology spread constraints for even pod placement across AZs in a .NET microservices setup.</li>
</ul>
<h4 id="best-practices">Best Practices</h4>
<ul>
<li><strong>Resource Requests/Limits</strong>: Always set them in Deployments—HPA uses them for calculations. For .NET, profile with <code>dotnet-counters</code> to tune (e.g., 200m CPU for idle API).</li>
<li><strong>Metrics Reliability</strong>: Enable Metrics Server; use Prometheus for custom ones. Avoid scaling on noisy metrics like network I/O.</li>
<li><strong>Graceful Scaling</strong>: Pair with Readiness Probes (e.g., /health/ready) to ensure new pods are traffic-ready before HPA counts them.</li>
<li><strong>Limits</strong>: Set min/max replicas to control costs; use VPA (Vertical Pod Autoscaler) alongside for resource optimization.</li>
<li><strong>Pitfalls</strong>: Over-scaling on latency (use custom metrics); ensure Services load-balance evenly.</li>
<li><strong>.NET Tip</strong>: Use Kestrel's config for concurrency; test scaling with tools like k6 for load simulation.</li>
</ul>
<p>This gets your .NET app resilient and cost-effective in K8s. Ready for Vertical Pod Autoscaler, or RBAC next? Fire away!</p>

    </div>
</body>
</html>"